{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_process_analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKP8/fradM0UBxX7NesT8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sihan827/2021_Winter_PE/blob/main/train_process_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR8RQKh5Y9_Y"
      },
      "source": [
        "# 훈련 과정 함수 정의\r\n",
        "def train(model, train_loader, optimizer, epochs):\r\n",
        "  # torch.nn.Module 클래스를 상속받은 모든 신경망 모델 클래스에는 train, eval 함수가 존재\r\n",
        "  # train함수를 실행하면 모델 클래스의 training 변수가 True가 됨\r\n",
        "  # 이를 통해 dropout이나 배치 정규화를 학습과정에서는 실행하고 테스트과정에서는 실행하지 않게 됨\r\n",
        "  model.train()\r\n",
        "  # 에폭은 전체 학습 데이터를 몇 번을 돌릴지의 횟수\r\n",
        "  for epoch in range(epochs):\r\n",
        "    # enumerate 함수를 통해 이터러블 객체인 train_loader에서 각 배치 데이터와 인덱스를 묶음\r\n",
        "    # for문 내에서 train_loader의 배치의 인덱스, 데이터와 라벨을 순회함 \r\n",
        "    for idx, (data, target) in enumerate(train_loader, 0):\r\n",
        "      # 불러온 데이터 텐서와 라벨 텐서를 cuda/cpu에서 연산하도록 설정\r\n",
        "      data, target = data.to(DEVICE), target.to(DEVICE)\r\n",
        "      # optimizer를 정의할 시 인자로 모델의 파라미터(가중치, 편향)을 받음\r\n",
        "      # zero_grad 함수를 호출하면 해당 파라미터들의 grad값이 0으로 초기화됨\r\n",
        "      # 만약 각 step마다 0으로 초기화하지 않는다면 \r\n",
        "      # 각 step마다 backward를 호출하여 계산된 파라미터들의 grad값이 계속 누적됨\r\n",
        "      # 이는 손실값이 감소하는 방향으로 가지 않을 수 있음\r\n",
        "      optimizer.zero_grad()\r\n",
        "      # 존재하는 모델로 데이터에 대한 결과를 예측함\r\n",
        "      output = model(data)\r\n",
        "      # 원하는 방법으로 손실함수를 계산\r\n",
        "      # 해당 경우에는 torch.nn.functional의 크로스 엔트로피 함수를 사용\r\n",
        "      # torch의 cross_entropy 함수는 로그 소프트맥스 + NLLLoss의 기능을 겸함\r\n",
        "      # 즉 모델 자체에서 소프트맥스를 따로 계산하지 않아도 인자로 전달하고 target으로 라벨을 전달하면\r\n",
        "      # 알아서 모델의 결과에 로그 소프트맥스를 적용한 후 \r\n",
        "      # target 라벨에 대하여 크로스 엔트로피 손실을 계산함\r\n",
        "      # 즉 loss에는 모델의 파라미터(가중치, 편향 등)를 포함한 손실값 계산식이 저장됨\r\n",
        "      loss = F.cross_entropy(output, target)\r\n",
        "      # 계산을 원하는 식에 backward 함수 호출 시 해당 식에 대하여 back propagation 시행\r\n",
        "      # (autograd가 True인)각 파라미터에 대한 grad 값이 계산됨\r\n",
        "      loss.backward()\r\n",
        "      # 선언한 optimizer의 step 함수 호출 시 \r\n",
        "      # backward로 계산된 각 파라미터의 grad 값을 이용하여 \r\n",
        "      # 선언한 optimizer에서 지정한 모델의 파라미터만 최적화시킴 \r\n",
        "      optimizer.step()\r\n",
        "      # 밑의 프린트문은 배치 60개마다 각 배치의 loss값의 평균을 출력하도록 한 것임\r\n",
        "      # 이를 통해 현재 학습이 손실값이 감소하는 방향으로 진행중인지 판단\r\n",
        "      # cross_entropy의 키워드 인자 중 reduction의 기본은 'mean'으로 \r\n",
        "      # 이는 손실값을 어떻게 출력할 것인지를 정하는 인자로 기본값인 각 배치의 손실값 평균을 출력함\r\n",
        "      # loss의 item 함수로 loss의 평균을 float형으로 출력\r\n",
        "      if idx % 60 == 0:\r\n",
        "        print('[%d %5d] loss : %f' % (epoch, idx, loss.item()))\r\n",
        "  print('done!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}